{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torch.utils.data  import DataLoader, Dataset, Sampler\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from rival10 import *\n",
    "RIVAL10_constants.set_rival10_dir(\"/home/ksas/Public/datasets/RIVAL10/\")\n",
    "\n",
    "def load_local_rival10(batch_size:int = 16, \n",
    "                        num_workers:int = 4, \n",
    "                        preprocess:Optional[transforms.Compose]=None):\n",
    "\n",
    "    trainset = LocalRIVAL10(train=True, \n",
    "                            masks_dict=True, \n",
    "                            transform=preprocess,\n",
    "                            verbose = \"key\")\n",
    "    testset = LocalRIVAL10(train=False, \n",
    "                            masks_dict=True, \n",
    "                            transform=preprocess,\n",
    "                            verbose = \"key\")\n",
    "\n",
    "    class_to_idx = {c: i for (i,c) in enumerate(RIVAL10_constants._ALL_CLASSNAMES)}\n",
    "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size,\n",
    "                                shuffle=False, num_workers=num_workers)\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size,\n",
    "                                shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    return trainset, testset, train_loader, test_loader, class_to_idx, idx_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(224),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "trainset, testset, train_loader, test_loader, class_to_idx, idx_to_class = load_local_rival10(preprocess = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_loader))\n",
    "\n",
    "for key in data.keys():\n",
    "    if isinstance(data[key], torch.Tensor):\n",
    "        print(f\"\\t{key}: \\n{data[key].size()}\")\n",
    "    else:\n",
    "        print(f\"\\t{key}: \\n{data[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import interactive\n",
    "\n",
    "def reduce_tensor_as_numpy(input:torch.Tensor) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input: [1, C, W, H] or [C, W, H]\n",
    "    \n",
    "    Returns:\n",
    "        [W, H, C]\n",
    "    \"\"\"\n",
    "    if input.size().__len__() == 4:\n",
    "        input.squeeze_(0)\n",
    "    return input.permute((1, 2, 0)).detach().cpu().numpy()\n",
    "                \n",
    "def show_image(images:torch.Tensor, comparison_images:torch.Tensor=None):\n",
    "    import torchvision\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if comparison_images is not None:\n",
    "        images = torch.cat((images, comparison_images), dim=3)\n",
    "\n",
    "    grid_img = torchvision.utils.make_grid(images, nrow=4, normalize=True)\n",
    "\n",
    "    plt.imshow(grid_img.permute(1, 2, 0)) \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def getAttMap(img, attn_map, blur=True):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.ndimage import filters\n",
    "    def normalize(x: np.ndarray) -> np.ndarray:\n",
    "        # Normalize to [0, 1].\n",
    "        x = x - x.min()\n",
    "        if x.max() > 0:\n",
    "            x = x / x.max()\n",
    "        return x\n",
    "\n",
    "    if blur:\n",
    "        attn_map = filters.gaussian_filter(attn_map, 0.02*max(img.shape[:2]))\n",
    "    # pos_mask = attn_map <= 0\n",
    "    attn_map = normalize(attn_map)\n",
    "    # attn_map[pos_mask] = 0\n",
    "    cmap = plt.get_cmap('jet')\n",
    "    attn_map_c = np.delete(cmap(attn_map), 3, 2)\n",
    "    attn_map = 1*(1-attn_map**0.7).reshape(attn_map.shape + (1,))*img + \\\n",
    "            (attn_map**0.7).reshape(attn_map.shape+(1,)) * attn_map_c\n",
    "    return attn_map\n",
    "\n",
    "def viz_attn_multiple(batch_X:torch.Tensor, attributions:list[torch.Tensor], blur=True, prefix:str=\"\", save_to:str=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    batch_X = reduce_tensor_as_numpy(batch_X)\n",
    "    attributions = [reduce_tensor_as_numpy(attribution) for attribution in attributions]\n",
    "    \n",
    "    attn_map = []\n",
    "    for attribution in attributions:\n",
    "        attn_map.append(getAttMap(batch_X, attribution.sum(2), blur))\n",
    "\n",
    "    \n",
    "    _, axes = plt.subplots(1, 1 + attn_map.__len__(), figsize=(50, 10))\n",
    "    axes[0].imshow(np.clip(batch_X, 0.0, 1.0))\n",
    "    for idx, map in  enumerate(attn_map):\n",
    "        axes[1 + idx].imshow(np.clip(map, 0.0, 1.0))\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    if save_to is not None:\n",
    "        os.makedirs(save_to, exist_ok=True)\n",
    "        plt.savefig(os.path.join(save_to, f\"{prefix}-attn_image.jpg\"), bbox_inches='tight', pad_inches=0.1)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# attr_template = None\n",
    "# for idx, data in enumerate(testset):\n",
    "#     if data[\"og_class_name\"] != \"plane\":\n",
    "#         continue\n",
    "\n",
    "#     if attr_template is None:\n",
    "#         attr_template = data[\"attr_labels\"]\n",
    "#     else:\n",
    "#         print((data[\"attr_labels\"] == attr_template).sum())\n",
    "    # print(f\"We're showing the No.{idx} sample\")\n",
    "    # viz_attn_multiple(data[\"img\"],\n",
    "    #     [data[\"attr_masks\"][1]],)\n",
    "    # import pdb; pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_sample_idx = [10, 19, 29, 35]\n",
    "print(RIVAL10_constants._ALL_ATTRS)\n",
    "for idx in abnormal_sample_idx:\n",
    "    data = testset[idx]\n",
    "    print(data[\"attr_labels\"])\n",
    "    print(data[\"changed_attrs\"])\n",
    "    viz_attn_multiple(data[\"img\"],\n",
    "        [data[\"attr_masks\"][i] for i in range(18)],)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pcbm_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
